# -*- coding: utf-8 -*-
"""setup2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T0nep7aaqkgO5m8UpbMtk1Tq23_EaG96
"""

import numpy as np
import pandas as pd
import json
import spacy


import requests
import urllib

def load_data(train_df):
    contexts = []
    questions = []
    answers = []
    ids = []
    for i in range(train_df['data'].shape[0]):
        topic = train_df['data'].iloc[i]['paragraphs']
        for sub_para in topic:
            context = sub_para['context']
            for q_a in sub_para['qas']:
                questions.append(q_a['question'])
                contexts.append(context)
                ids.append(q_a['id'])
                if q_a['is_impossible'] is False:
                    answers.append(q_a['answers'][0])
                else:
                    answer = {}
                    answer['answer_start'] = 0
                    answer['text'] = 'no-answer'
                    answers.append(answer)

    return contexts, questions, answers, ids



train_response = urllib.request.urlopen("https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json")
train_raw = pd.read_json(train_response)

#val_response = urllib.request.urlopen("https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json")
#val_raw = pd.read_json(val_response)

train_contexts, train_questions, train_answers, train_ids = load_data(train_raw)
#val_contexts, val_questions, val_answers, val_ids = load_data(val_raw)


"""
there is only start index in SQuAD dataset. add end_index
"""
def add_end_idx(answers, contexts):
    for answer, context in zip(answers,contexts):
        if answer['text'] == "no-answer":
            answer['answer_end'] = 0
        else:
            gold_text = answer['text']
            start_idx = answer['answer_start']
            end_idx = start_idx + len(gold_text)
            #sometimes SQuAD answers are off by a character or two 
            if context[start_idx:end_idx] == gold_text:
                answer['answer_end'] = end_idx
            elif context[start_idx-1:end_idx-1] == gold_text:
                answer['answer_start'] = start_idx - 1
                answer['answer_end'] = end_idx - 1
            elif context[start_idx-2: end_idx-2] == gold_text:
                answer['answer_start'] = start_idx - 2
                answer['answer_end'] = end_idx - 2

add_end_idx(train_answers, train_contexts)
#add_end_idx(val_answers, val_contexts)

from transformers import BertTokenizer, BertForQuestionAnswering, BertTokenizerFast
import torch

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

"""
small_train_contexts = train_contexts
small_train_questions = train_questions
small_train_answers = train_answers

small_val_contexts = val_contexts
small_val_questions = val_questions
small_val_answers = val_answers
"""

train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)
#val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)

"""
start_position in SQuAD is character_position
transfer to token_position
"""
def add_token_positions(encodings, answers, tokenizer):
    start_positions = []
    end_positions = []
    for i in range(len(answers)):
      if answers[i]['text'] == 'no-answer':
        start_positions.append(0)
        end_positions.append(0)
      else:
        start_positions.append(encodings.char_to_token(i,answers[i]['answer_start']))
        end_positions.append(encodings.char_to_token(i,answers[i]['answer_end']-1))
        #if none, the answer span has been truncated
        if start_positions[-1] is None:
            start_positions[-1] = tokenizer.model_max_length
        if end_positions[-1] is None:
            end_positions[-1] = tokenizer.model_max_length

    encodings.update({'start_positions':start_positions,'end_positions':end_positions})

add_token_positions(train_encodings,train_answers,tokenizer)
#add_token_positions(val_encodings,val_answers,tokenizer)

class SquadDataset(torch.utils.data.Dataset):
  def __init__(self,encodings):
    self.encodings = encodings
  def __getitem__(self,idx):
    return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}
  def __len__(self):
    return len(self.encodings.input_ids)

small_train_dataset = SquadDataset(train_encodings)
#small_val_dataset = SquadDataset(val_encodings)

from torch.utils.data import DataLoader
from transformers import AdamW

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model = BertForQuestionAnswering.from_pretrained("bert-base-uncased",return_dict=True)

model.to(device)
model.train()

train_loader = DataLoader(small_train_dataset,batch_size=16,shuffle=True)

optim = AdamW(model.parameters(),lr=5e-5)

"""
training process
"""
curr_loss = 0
all_loss = []
iteration = 0
plot_every = 50
log_every = 50
for epoch in range(3):
  for batch in train_loader:
    optim.zero_grad()
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    start_positions = batch['start_positions'].to(device)
    end_positions = batch['end_positions'].to(device)
    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions,
                    end_positions=end_positions)
    loss = outputs[0]
    iteration += 1
    #all_loss.append(loss.item())
    
    curr_loss += curr_loss + loss.item()
    if iteration % plot_every == 0:
        all_loss.append(curr_loss/plot_every)
        curr_loss = 0
    
    if iteration % log_every == 0:
        print("iteration:",iteration," loss:",loss.item())
    loss.backward()
    
    optim.step()

model.eval()

torch.save(model.state_dict(),'baseModel2.pt')

with open('loss.txt', 'w') as filehandle:
    for loss in all_loss:
      filehandle.write('%s\n' % loss)

"""
nlp = spacy.blank("en")

def word_tokenize(sent):
    doc = nlp(sent)
    return [token.text for token in doc]

import collections

def get_F1_score(golden_answer, prediction):
    golden_tokens = word_tokenize(golden_answer)
    pred_tokens = word_tokenize(prediction)
    
    common = collections.Counter(golden_tokens) & collections.Counter(pred_tokens)
    num_same = sum(common.values())

    if len(pred_tokens) == 0 or len(golden_tokens) == 0:  #ignore the case of no-answer at this stage
        return 0

    precision = num_same / len(pred_tokens)
    recall = num_same / len(golden_tokens)
    if precision + recall == 0:
        f1 = 0
    else:
        f1 = (2 * precision * recall) / (precision + recall)
    return f1

def evaluate(eval_dataset, answers):
    n = len(eval_dataset)
    exact_match = 0
    f1_sum = 0
    for i in range(n):
        input_ids = eval_dataset[i]['input_ids'].to(device)
        attention_mask = eval_dataset[i]['attention_mask'].to(device)
        golden_answer = answers[i]['text']
        with torch.no_grad():
            output = model(torch.unsqueeze(input_ids,0), torch.unsqueeze(attention_mask,0))
        start = torch.argmax(output[0][0])
        end = torch.argmax(output[1][0])
        tokens = tokenizer.convert_ids_to_tokens(input_ids)
        prediction = ' '.join(tokens[start:end+1])

        #exact match
        if(prediction == golden_answer):
            exact_match = exact_match + 1

        #F1_score
        f1_sum = f1_sum + get_F1_score(golden_answer, prediction)
        
        
    accuracy = exact_match/n
    f1 = f1_sum / n
    return accuracy, f1

accuracy, f1 = evaluate(small_val_dataset, small_val_answers)

print("...................")
print("accuracy:",accuracy)
print("f1",f1)

"""

